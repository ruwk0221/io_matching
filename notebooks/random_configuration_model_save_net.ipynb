{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "atomic-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('__file__'))))\n",
    "import jReversion as jR\n",
    "from LDOI import BooleanDOI_processing as BDOIp\n",
    "from LDOI import qm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import concurrent.futures\n",
    "import par_helper as ph\n",
    "import random\n",
    "import robustness as rb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "documentary-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_configuration_network_canalizing(din, dout, bias=0.5):\n",
    "    '''\n",
    "    din: list of in-degree\n",
    "    dout: list of out-degree\n",
    "    bias: threshold for random Boolean logic\n",
    "    The code is originated from the read_ndetwork() by Colin Campbell.\n",
    "    '''\n",
    "    def clean_states(x):\n",
    "        #cleans binary representation of node input states\n",
    "        out=x[2:]                                                               # Strip leading 0b\n",
    "        return '0'*(len(inf)-len(out))+out                                      # Append leading 0's as needed\n",
    "\n",
    "    while True:\n",
    "        g = nx.DiGraph(nx.directed_configuration_model(din, dout))\n",
    "        \n",
    "        input_node = [node for node in g.nodes if g.in_degree(node) == 0]\n",
    "        output_node = [node for node in g.nodes if g.out_degree(node) == 0]\n",
    "        if len(input_node) == 0:              # at least one input node\n",
    "            random_node = random.randrange(n)\n",
    "            g.remove_edges_from(list(g.in_edges(random_node)))\n",
    "            input_node = [node for node in g.nodes if g.in_degree(node) == 0]\n",
    "            \n",
    "        if len(output_node) == 0:            # at least one output node\n",
    "            random_node = random.randrange(n)\n",
    "            g.remove_edges_from(list(g.out_edges(random_node)))\n",
    "            output_node = [node for node in g.nodes if g.out_degree(node) == 0]\n",
    "        \n",
    "        if len(g.subgraph(input_node + output_node).edges) > 0: continue # output nodes are connected to input nodes\n",
    "                    \n",
    "        if nx.is_weakly_connected(g): break\n",
    "\n",
    "    for n in g.nodes:\n",
    "        inf = list(g.predecessors(n))\n",
    "        if len(inf) > 0: \n",
    "            g.nodes[n]['update_nodes'] = inf.copy()\n",
    "            g.nodes[n]['update_rules'] = {}\n",
    "\n",
    "            bool_states = map(bin,range(2**len(inf)))\n",
    "            bool_states = map(clean_states,bool_states)\n",
    "            canalizing_variable = random.randrange(len(inf))\n",
    "            canalizing_value = int(random.random() < 0.5)\n",
    "            canalized_value = int(random.random() < 0.5)\n",
    "            for j in bool_states:\n",
    "                if j[canalizing_variable] == str(canalizing_value):\n",
    "                    g.nodes[n]['update_rules'][j] = canalized_value\n",
    "                else: \n",
    "                    g.nodes[n]['update_rules'][j] = int(random.random() < bias)    # Store outcome for every possible input\n",
    "        else:\n",
    "#             g.add_edge(n, n)\n",
    "            g.nodes[n]['update_nodes'] = [n]\n",
    "            g.nodes[n]['update_rules'] = {'0': 0, '1': 1}\n",
    "\n",
    "    return g,list(g.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abandoned-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "networkModel =[ 'bortezomib',\n",
    "                # 'igvh',\n",
    "                'apoptosis',\n",
    "                # 'aurora',\n",
    "                #'bt474_long',\n",
    "                'bt474_short',\n",
    "                # 'cd4t',\n",
    "                'colitis',\n",
    "                'death',\n",
    "                # 'egfr',\n",
    "                # 'erbb',\n",
    "                # 'fa_brca',\n",
    "                # 'fa_check',\n",
    "                #'hcc1954_long',\n",
    "                'hcc1954_short',\n",
    "                'hgf',\n",
    "                'mammalian',\n",
    "                # 'mammalian_2006',\n",
    "                'mapk',\n",
    "                'oxidative',\n",
    "                # 'pro_inflammatory',\n",
    "                'fibroblasts',\n",
    "                #'skbr3_long',\n",
    "                'skbr3_short',\n",
    "                'tlgl_2008',\n",
    "                'tlgl_2011',\n",
    "                # 'tlgl_2011_reduced',\n",
    "                # 'prostate',\n",
    "                'migration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "separated-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Model in networkModel:\n",
    "    Prefix, Suffix = 'n', 'n'\n",
    "    TEMP = jR.cellcollective(Model, Prefix, Suffix, directory='../')\n",
    "\n",
    "    BooleanRuleFileName = TEMP['BooleanRule_filename']\n",
    "    network_name = TEMP['network_name']\n",
    "\n",
    "    NumInputs = TEMP['num_inputs']\n",
    "    NumInputConditions = TEMP['num_input_conditions']\n",
    "\n",
    "    InputConditions = TEMP['input_conditions']\n",
    "\n",
    "    OutputNodes = TEMP['output_nodes']\n",
    "    InputNodes = TEMP['input_nodes']\n",
    "    \n",
    "    Mapping = TEMP['mapping']\n",
    "    InverseMapping = TEMP['inverse_mapping']\n",
    "    GRead = TEMP['Gread']\n",
    "    ReadNodes = TEMP['read_nodes']\n",
    "    \n",
    "    GCopy = GRead.copy()\n",
    "    GCopy.remove_edges_from(nx.selfloop_edges(GCopy))\n",
    "    Din = list(d for n, d in GCopy.in_degree())\n",
    "    Dout = list(d for n, d in GCopy.out_degree())\n",
    "    \n",
    "#     result_pd = pd.DataFrame(columns=['network_idx', 'size_of_network', 'num_link', 'C0', \n",
    "#                                   'C1', 'C2', 'C3', 'robustness_pa', 'robustness_ip', \n",
    "#                                   'redundancy'])\n",
    "#     for idx in range(0, 500):\n",
    "#         temp = r3_random_configuration_network_canalizing(GRead, bias=0.5, prefix='n', suffix='n', num_worker=10, sampling_num=10)\n",
    "#         temp['network_idx'] = idx\n",
    "#         result_pd = result_pd.append(temp, ignore_index=True)\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(gen_random_configuration_network_canalizing, din=Din, dout=Dout, bias=0.5): network_idx for network_idx in range(500)}\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        network_idx = futures[future]\n",
    "        temp, _ = future.result()\n",
    "#         temp['network_idx'] = network_idx\n",
    "# configuration_models\n",
    "        with open('../networks/configuration_models/'+Model+'_' + str(network_idx) + '.pck', 'wb') as f:\n",
    "            pickle.dump(temp, f)\n",
    "#     result_pd.to_csv('../data/'+ Model + '_canalizing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "first-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def r3_test(g, bias=0.5, prefix='n', suffix='n', num_worker=5, sampling_num=10):\n",
    "    \n",
    "#     read_nodes = list(g.nodes)\n",
    "#     mapping = {}  # nodename to number index\n",
    "#     inverse_mapping = {}  # number index to nodename\n",
    "#     read_nodes_dict = {}\n",
    "#     inverse_read_nodes_dict = {}\n",
    "#     for i, node in enumerate(read_nodes):\n",
    "#         index = prefix + str(i) + suffix\n",
    "#         mapping[str(node)] = index\n",
    "#         inverse_mapping[index] = str(node)\n",
    "#         mapping['~' + str(node)] = '~' + index\n",
    "#         inverse_mapping['~' + index] = '~' + str(node)\n",
    "#         read_nodes_dict[i] = str(node)\n",
    "#         inverse_read_nodes_dict[str(node)] = i\n",
    "    \n",
    "#     input_nodes = [node for node in g.nodes if g.in_degree(node) == 0]\n",
    "#     output_nodes = [node for node in g.nodes if g.out_degree(node) == 0]\n",
    "    \n",
    "#     num_inputs = len(input_nodes)\n",
    "#     num_input_conditions = 2 ** num_inputs\n",
    "#     input_conditions = np.ndarray((num_inputs, 2), dtype=object)\n",
    "#     for idx, input_node in enumerate(input_nodes):\n",
    "#         input_conditions[idx, 0] = '~' + str(input_node)\n",
    "#         input_conditions[idx, 1] = str(input_node)\n",
    "    \n",
    "    \n",
    "#     output_nodes_ex = []\n",
    "#     output_nodes_ex.extend([read_nodes_dict[idx] for idx in output_nodes])\n",
    "#     output_nodes_ex.extend(['~'+read_nodes_dict[idx] for idx in output_nodes])\n",
    "#     input_nodes_ex = input_conditions.reshape(num_inputs * 2, ).tolist()\n",
    "    \n",
    "#     GExpanded = ph.par_get_expanded_network(g, prefix=prefix, suffix=suffix, worker=num_worker)\n",
    "# #     GExpanded = BDOIp.Get_expanded_network(g, prefix=prefix, suffix=suffix)\n",
    "#     TempGIOW = jR.get_input_output_relation(GExpanded, mapping, inverse_mapping, input_conditions, output_nodes_ex,\n",
    "#                                             constant_nodes=[])\n",
    "#     LDOIs = TempGIOW['LDOIs']\n",
    "#     GeneLDOIs = TempGIOW['gene_LDOIs']\n",
    "#     Conflicts = TempGIOW['conflicts']\n",
    "#     GeneConflicts = TempGIOW['gene_conflicts']\n",
    "#     IORelation = TempGIOW['io_relation']\n",
    "#     GRemained = TempGIOW['G_remained']\n",
    "\n",
    "#     R0 = jR.identifying_r0_mutations_ldoi(GExpanded, output_nodes_ex, mapping, inverse_mapping, input_conditions, IORelation)\n",
    "#     # DM = jR.identifying_disconnecting_mutations(GExpanded, InputNodes, OutputNodes, Mapping, InverseMapping)\n",
    "#     R1 = jR.identifying_r1_mutations_ldoi(GExpanded, output_nodes_ex, mapping, inverse_mapping, input_conditions, IORelation)\n",
    "# #     IIDC = jR.identifying_input_independent_canalizing_mutations(GExpanded, OutputNodes, Mapping, InverseMapping)\n",
    "# #     UN = jR.identifying_input_unreachable_nodes(GExpanded, OutputNodes, Mapping, InverseMapping, InputConditions)\n",
    "#     RN = jR.identifying_rn_mutations(GExpanded, mapping, inverse_mapping, input_nodes_ex, output_nodes_ex, input_conditions,\n",
    "#                                      IORelation,\n",
    "#                                      R0['ineffective_mutations'], R1['r1_mutations'])\n",
    "    \n",
    "#     NodeList = set(read_nodes_dict.values())\n",
    "#     NodeList.difference_update(input_nodes)\n",
    "#     NodeList.difference_update(output_nodes)\n",
    "#     C0, C1, C2, C3 = [], [], [], []\n",
    "#     for NODE in NodeList:\n",
    "#         negNODE = '~' + NODE\n",
    "#         if R0['ineffective'][NODE] and R0['ineffective'][negNODE]:\n",
    "# #             nodeClass = 'C0'\n",
    "#             C0.append(NODE)\n",
    "#         elif NODE in RN['rn_mutations']:\n",
    "#             if RN['rn_mutations'][NODE] == 'R1':\n",
    "# #                 nodeClass = 'C1'\n",
    "#                 C1.append(NODE)\n",
    "# #                 canalizing = IIDC['iid_canalizing'][NODE]\n",
    "# #                 unreachable = UN['input_unreachable'][NODE]\n",
    "#             else:\n",
    "# #                 nodeClass = 'C2'\n",
    "#                 C2.append(NODE)\n",
    "# #                 canalizing = IIDC['iid_canalizing'][NODE]\n",
    "# #                 unreachable = UN['input_unreachable'][NODE]\n",
    "#         elif negNODE in RN['rn_mutations']:\n",
    "#             if RN['rn_mutations'][negNODE] == 'R1':\n",
    "# #                 nodeClass = 'C1'\n",
    "#                 C1.append(NODE)\n",
    "# #                 canalizing = IIDC['iid_canalizing'][NODE]\n",
    "# #                 unreachable = UN['input_unreachable'][NODE]\n",
    "#         else:\n",
    "# #             nodeClass = 'C3'\n",
    "#             C3.append(NODE)\n",
    "# #             canalizing = IIDC['iid_canalizing'][NODE]\n",
    "# #             unreachable = UN['input_unreachable'][NODE]\n",
    "    \n",
    "\n",
    "#     robustness_pa = rb.robustness_primary_attractor(g_read=g, state_num=2**sampling_num)\n",
    "#     robustness_ip = rb.robustness_initial_perturbation(g_read=g, state_num=2**sampling_num)\n",
    "    \n",
    "#     robustness = []\n",
    "#     for key, val in robustness_pa.items():\n",
    "#         robustness.append(val['oe'])\n",
    "#         robustness.append(val['ko'])\n",
    "# #     print(np.mean(robustness))\n",
    "    \n",
    "    \n",
    "#     # deterministic_IO\n",
    "#     empty_io = [x for x in IORelation[0,:] if len(x) == 0]\n",
    "#     len_io = [len(x) for x in IORelation[0,:]]\n",
    "#     empty_io_ratio = len(empty_io) / num_input_conditions\n",
    "#     deterministic_io_ratio = sum(len_io) / (num_input_conditions * len(output_nodes))\n",
    "    \n",
    "    \n",
    "#     LDOI_union = set()\n",
    "#     for LDOI in LDOIs[0,:]:\n",
    "#         LDOI_union = LDOI_union.union(LDOI)\n",
    "#     LDOI_union = LDOI_union.union([mapping[x] for x in input_conditions.reshape((2*num_inputs,))])\n",
    "#     GSub = nx.subgraph(GExpanded, LDOI_union)\n",
    "#     io_count = 0\n",
    "#     io_pathway_count = 0\n",
    "#     for i in range(num_input_conditions):\n",
    "#         # G_copied = G_expanded.copy()\n",
    "#         BinaryBit = np.binary_repr(i, width=num_inputs)\n",
    "#         InputCondition = [input_conditions[x, int(BinaryBit[x])] for x in range(num_inputs)]\n",
    "#         # source_nodes = input_condition + constant_nodes\n",
    "#         Sources = set([x for x in InputCondition])\n",
    "#         # for node in source:\n",
    "#         #     G_copied.remove_edges_from(list(G_copied.in_edges(node)))\n",
    "#         #     G_copied.remove_edges_from(list(G_copied.in_edges(BDOId.Negation_in_expanded(node))))\n",
    "\n",
    "#         Targets = set([x for x in IORelation[0, i]])\n",
    "#         if len(Targets) > 0:\n",
    "#             for s in Sources:\n",
    "#                 for t in Targets:\n",
    "#                     if nx.has_path(G=g, source=inverse_read_nodes_dict[s.replace('~', '')], target=inverse_read_nodes_dict[t.replace('~', '')]):\n",
    "#                         io_count += 1\n",
    "#                         len_shortest_path = nx.shortest_path_length(G=g, source=inverse_read_nodes_dict[s.replace('~', '')], target=inverse_read_nodes_dict[t.replace('~', '')])\n",
    "#                         for p in nx.all_simple_paths(G=g, source=inverse_read_nodes_dict[s.replace('~', '')], target=inverse_read_nodes_dict[t.replace('~', '')], cutoff=2*len_shortest_path):\n",
    "#                             io_pathway_count += 1\n",
    "\n",
    "#     if io_count == 0:\n",
    "#         redundancy = -1\n",
    "#     else:\n",
    "#         redundancy = io_pathway_count/float(io_count)\n",
    "    \n",
    "#     return {'size_of_network': len(g), 'num_link': len(g.edges), 'num_input': len(input_nodes), 'num_output': len(output_nodes),\n",
    "#             'C0': len(C0), 'C1': len(C1), 'C2': len(C2), 'C3': len(C3), \n",
    "#             'robustness_pa': np.mean(robustness), 'robustness_ip': robustness_ip,\n",
    "#             'empty_io_ratio': empty_io_ratio, 'deterministic_io_ratio':deterministic_io_ratio,\n",
    "#             'redundancy': redundancy}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
